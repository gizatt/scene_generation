{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gizatt/projects/scene_generation/inverse_graphics/synthetic_scene_database_loader.py:24: _DrakeImportWarning: \n",
      "You may have already (directly or indirectly) imported `torch` which uses\n",
      "`RTLD_GLOBAL`. Using `RTLD_GLOBAL` may cause symbol collisions which manifest\n",
      "themselves in bugs like \"free(): invalid pointer\". Please consider importing\n",
      "`pydrake` (and related C++-wrapped libraries like `cv2`, `open3d`, etc.)\n",
      "*before* importing `torch`. For more details, see:\n",
      "https://github.com/pytorch/pytorch/issues/3059#issuecomment-534676459\n",
      "\n",
      "  from scene_generation.utils.type_convert import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gizatt/miniconda3/envs/py36_pyro/lib/python3.6/site-packages/torchvision-0.6.0a0+fb562f5-py3.6-linux-x86_64.egg/torchvision/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import community as community_louvain\n",
    "from copy import deepcopy\n",
    "from collections import OrderedDict\n",
    "import cv2\n",
    "import copy\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import point_cloud_utils as pcu\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from skimage.transform import resize\n",
    "from skimage.color import rgb2gray\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import dense_correspondence_manipulation.utils.utils as utils\n",
    "\n",
    "import meshcat\n",
    "import meshcat.geometry as g\n",
    "\n",
    "import detectron2.utils.comm as comm\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.config import CfgNode as CN\n",
    "from detectron2.data import (\n",
    "    MetadataCatalog,\n",
    "    build_detection_test_loader,\n",
    "    build_detection_train_loader,\n",
    ")\n",
    "from detectron2.data.datasets import load_coco_json\n",
    "from detectron2.engine import DefaultTrainer, DefaultPredictor, default_argument_parser, default_setup, launch\n",
    "from detectron2.evaluation import inference_on_dataset, COCOEvaluator\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.utils.events import EventStorage\n",
    "from detectron2.utils.logger import setup_logger\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "\n",
    "import scene_generation.inverse_graphics.synthetic_scene_database_loader as loader\n",
    "from scene_generation.inverse_graphics.synthetic_scene_database_loader import XenRCNNMapper\n",
    "import scene_generation.inverse_graphics.keypoint_mcmc.roi_heads as roi_heads\n",
    "from scene_generation.inverse_graphics.keypoint_mcmc.particle_filter_icp import *\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "\n",
    "DATA_ROOT = \"/home/gizatt/data/generated_cardboard_envs/\"\n",
    "DETECTRON_ROOT = \"/home/gizatt/tools/detectron2/\"\n",
    "\n",
    "\n",
    "class InstanceCloud():\n",
    "    def __init__(self, pts, colors, descriptors):\n",
    "        self.pts = pts\n",
    "        self.descriptors = descriptors\n",
    "        self.colors = colors\n",
    "    def get_augmented_pts(self, descriptor_factor=1.):\n",
    "        return np.vstack([self.pts, descriptor_factor*self.descriptors])\n",
    "    \n",
    "%matplotlib inline\n",
    "def cv2_imshow(im):\n",
    "    plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "\n",
    "print(torchvision.__file__)\n",
    "\n",
    "load_dict = torch.load(\"run_on_all_records.pt\")\n",
    "all_instance_clouds_by_record = load_dict[\"all_instance_clouds_by_record\"]\n",
    "affinities_by_record = load_dict[\"affinities_by_record\"]\n",
    "clusters_by_record = load_dict[\"clusters_by_record\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can open the visualizer by visiting the following URL:\n",
      "http://127.0.0.1:7000/static/\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import copy\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "clusters = []\n",
    "for c in list(clusters_by_record.values())[:10]:\n",
    "    clusters += c\n",
    "cluster = clusters[25]\n",
    "    \n",
    "vis = meshcat.Visualizer(zmq_url=\"tcp://127.0.0.1:6000\")\n",
    "vis.delete()\n",
    "\n",
    "\n",
    "def compute_unit_box_descriptors(pts, normals):\n",
    "    # Descriptor is the viridis color coding\n",
    "    # of the distance to the center of each pt's face.\n",
    "    face_local = np.abs(pts)*(normals == 0.0)*2\n",
    "    dist = np.linalg.norm(face_local, axis=0)/1.414\n",
    "    return torch.tensor(cm.get_cmap('viridis')(dist).astype(np.float32).T[:3, :])\n",
    "    \n",
    "# Finally, take a cluster + do ICP on it\n",
    "model_pts, model_normals = make_unit_box_pts_and_normals(N=2500)\n",
    "model_colors = np.zeros((3, model_pts.shape[1]))\n",
    "model_colors[0, :] = 1.\n",
    "model_descriptors = compute_unit_box_descriptors(model_pts, model_normals)\n",
    "model_pts = torch.tensor(np.dot(np.diag([0.3, 0.35, 0.35]), model_pts.numpy()))\n",
    "\n",
    "scene_offset = -np.mean(cluster.pts.numpy(), axis=1) + np.array([1., 0., 0.])\n",
    "vis[\"scene\"].set_object(\n",
    "    g.PointCloud(position=(cluster.pts.numpy().T + scene_offset).T,\n",
    "                 color=cluster.colors.numpy(),\n",
    "                 size=0.005))\n",
    "    \n",
    "\n",
    "if (0):\n",
    "    # Animate drawing correspondences with this scene cloud\n",
    "    for x in np.linspace(0., 1., 100):\n",
    "        target_color = cm.get_cmap('viridis')(x)[:3]\n",
    "        model_inds = np.linalg.norm(model_descriptors.numpy().T - target_color, axis=1) < 0.1\n",
    "        scene_inds = np.linalg.norm(cluster.descriptors.numpy().T - target_color, axis=1) < 0.1\n",
    "        vis[\"scene_highlight\"].set_object(\n",
    "            g.PointCloud(position=(cluster.pts.numpy()[:, scene_inds].T + scene_offset).T,\n",
    "                         color=cluster.descriptors.numpy()[:, scene_inds],\n",
    "                         size=0.02))\n",
    "        vis[\"model\"].set_object(\n",
    "        g.PointCloud(position=model_pts.numpy()[:, model_inds],\n",
    "                     color=model_descriptors.numpy()[:, model_inds],\n",
    "                     size=0.02))\n",
    "        time.sleep(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Point-to-plane ICP registration is applied on original point\n",
      "   clouds to refine the alignment. Distance threshold 0.02.\n",
      "registration::RegistrationResult with fitness=4.160000e-02, inlier_rmse=5.726449e-03, and correspondence_set size of 104\n",
      "Access transformation to get result.\n",
      "[[ 1.   -0.06 -0.08  0.25]\n",
      " [ 0.06  1.   -0.03  0.24]\n",
      " [ 0.08  0.02  1.    0.36]\n",
      " [ 0.    0.    0.    1.  ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_o3d = o3d.geometry.PointCloud()\n",
    "model_o3d.points = o3d.utility.Vector3dVector(model_pts.T)\n",
    "model_o3d.normals = o3d.utility.Vector3dVector(model_normals.T)\n",
    "model_o3d.colors = o3d.utility.Vector3dVector(model_descriptors.T)\n",
    "\n",
    "scene_o3d = o3d.geometry.PointCloud()\n",
    "scene_o3d.points = o3d.utility.Vector3dVector(cluster.pts.T)\n",
    "scene_o3d.colors = o3d.utility.Vector3dVector(cluster.descriptors.T)\n",
    "scene_o3d.estimate_normals()\n",
    "\n",
    "\n",
    "scene_o3d, ind = scene_o3d.remove_statistical_outlier(nb_neighbors=40,\n",
    "                                              std_ratio=1.0)\n",
    "\n",
    "\n",
    "# draw initial alignment\n",
    "current_transformation = np.identity(4)\n",
    "current_transformation[:3, 3] = np.mean(cluster.pts.numpy(), axis=1)\n",
    "\n",
    "print(\"2. Point-to-plane ICP registration is applied on original point\")\n",
    "print(\"   clouds to refine the alignment. Distance threshold 0.02.\")\n",
    "result_icp = o3d.registration.registration_icp(\n",
    "        model_o3d, scene_o3d, 0.01, current_transformation,\n",
    "        o3d.registration.TransformationEstimationPointToPlane())\n",
    "print(result_icp)\n",
    "\n",
    "tf = np.asarray(result_icp.transformation)\n",
    "print(tf)\n",
    "\n",
    "vis[\"scene\"].set_object(\n",
    "    g.PointCloud(position=np.asarray(scene_o3d.points).T,\n",
    "                 color=np.asarray(scene_o3d.colors).T,\n",
    "                 size=0.01))\n",
    "model_pts_tf = (np.dot(tf[:3, :3], np.asarray(model_o3d.points).T).T + tf[:3, 3]).T\n",
    "vis[\"model\"].set_object(\n",
    "    g.PointCloud(position=model_pts_tf,\n",
    "                 color=np.asarray(model_o3d.colors).T,\n",
    "                 size=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Colored point cloud registration\n",
      "[100, 0.05, 0]\n",
      "3-1. Downsample with a voxel size 0.05\n",
      "3-2. Estimate normal.\n",
      "3-3. Applying colored point cloud registration\n",
      "registration::RegistrationResult with fitness=1.000000e+00, inlier_rmse=8.317731e-02, and correspondence_set size of 268\n",
      "Access transformation to get result.\n",
      "[50, 0.01, 1]\n",
      "3-1. Downsample with a voxel size 0.01\n",
      "3-2. Estimate normal.\n",
      "3-3. Applying colored point cloud registration\n",
      "registration::RegistrationResult with fitness=5.626267e-01, inlier_rmse=1.181689e-02, and correspondence_set size of 1388\n",
      "Access transformation to get result.\n",
      "[50, 0.005, 2]\n",
      "3-1. Downsample with a voxel size 0.01\n",
      "3-2. Estimate normal.\n",
      "3-3. Applying colored point cloud registration\n",
      "registration::RegistrationResult with fitness=4.984000e-01, inlier_rmse=5.280888e-03, and correspondence_set size of 1246\n",
      "Access transformation to get result.\n",
      "[[-1.   -0.    0.    0.17]\n",
      " [ 0.   -1.    0.01  0.2 ]\n",
      " [ 0.    0.01  1.    0.32]\n",
      " [ 0.    0.    0.    1.  ]]\n"
     ]
    }
   ],
   "source": [
    "voxel_radius = [0.05, 0.01, 0.005]\n",
    "max_iter = [100, 50, 50]\n",
    "initial_tf = np.identity(4)\n",
    "from scipy.stats import special_ortho_group\n",
    "initial_tf[:3, :3] = special_ortho_group.rvs(3)\n",
    "initial_tf[:3, 3] = np.mean(cluster.pts.numpy(), axis=1) + np.random.random(3)*0.1-0.05\n",
    "current_transformation = initial_tf.copy()\n",
    "source = model_o3d\n",
    "target = scene_o3d\n",
    "print(\"3. Colored point cloud registration\")\n",
    "for scale in range(3):\n",
    "    iter = max_iter[scale]\n",
    "    radius = voxel_radius[scale]\n",
    "    print([iter, radius, scale])\n",
    "\n",
    "    print(\"3-1. Downsample with a voxel size %.2f\" % radius)\n",
    "    source_down = source.voxel_down_sample(radius)\n",
    "    target_down = target.voxel_down_sample(radius)\n",
    "    print(\"3-2. Estimate normal.\")\n",
    "    source_down.estimate_normals(\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=radius * 2, max_nn=30))\n",
    "    target_down.estimate_normals(\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=radius * 2, max_nn=30))\n",
    "\n",
    "    print(\"3-3. Applying colored point cloud registration\")\n",
    "    result_icp = o3d.registration.registration_colored_icp(\n",
    "        source_down, target_down, max_correspondence_distance=radius*4,\n",
    "        init=current_transformation,\n",
    "        criteria=o3d.registration.ICPConvergenceCriteria(relative_fitness=1e-6,\n",
    "                                                relative_rmse=1e-6,\n",
    "                                                max_iteration=iter))\n",
    "    current_transformation = result_icp.transformation\n",
    "    print(result_icp)\n",
    "\n",
    "tf = np.asarray(result_icp.transformation)\n",
    "print(tf)\n",
    "\n",
    "vis[\"scene\"].set_object(\n",
    "    g.PointCloud(position=np.asarray(scene_o3d.points).T,\n",
    "                 color=np.asarray(scene_o3d.colors).T,\n",
    "                 size=0.01))\n",
    "model_pts_tf = (np.dot(tf[:3, :3], np.asarray(model_o3d.points).T).T + tf[:3, 3]).T\n",
    "vis[\"model\"].set_object(\n",
    "    g.PointCloud(position=model_pts_tf,\n",
    "                 color=np.asarray(model_o3d.colors).T,\n",
    "                 size=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPFH: (36, 1000)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_point_cloud(pcd, voxel_size):\n",
    "    print(\":: Downsample with a voxel size %.3f.\" % voxel_size)\n",
    "    pcd_down = pcd # pcd.voxel_down_sample(voxel_size)\n",
    "\n",
    "    radius_normal = voxel_size * 2\n",
    "    print(\":: Estimate normal with search radius %.3f.\" % radius_normal)\n",
    "    pcd_down.estimate_normals(\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=radius_normal, max_nn=30))\n",
    "\n",
    "    radius_feature = voxel_size * 5\n",
    "    print(\":: Compute FPFH feature with search radius %.3f.\" % radius_feature)\n",
    "    pcd_fpfh = o3d.registration.compute_fpfh_feature(\n",
    "        pcd_down,\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=radius_feature, max_nn=100))\n",
    "    # Append colors to FPFH\n",
    "    features = np.vstack(\n",
    "        [np.asarray(pcd_fpfh.data),\n",
    "         np.asarray(pcd_down.colors).T])\n",
    "    pcd_fpfh.resize(features.shape[0], features.shape[1])\n",
    "    pcd_fpfh.data = features\n",
    "    return pcd_down, pcd_fpfh\n",
    "\n",
    "def prepare_dataset(source, target, voxel_size):\n",
    "    print(\":: Load two point clouds and disturb initial pose.\")\n",
    "    trans_init = np.identity(4)\n",
    "    trans_init[:3, :3] = special_ortho_group.rvs(3)\n",
    "    trans_init[:3, 3] = np.mean(np.asarray(source.points), axis=0) + np.random.random(3)*0.5-0.25\n",
    "    source.transform(trans_init)\n",
    "\n",
    "    source_down, source_fpfh = preprocess_point_cloud(source, voxel_size)\n",
    "    target_down, target_fpfh = preprocess_point_cloud(target, voxel_size)\n",
    "    return source_down, target_down, source_fpfh, target_fpfh\n",
    "\n",
    "def execute_fast_global_registration(source_down, target_down, source_fpfh,\n",
    "                                     target_fpfh, voxel_size,\n",
    "                                     distance_threshold=0.01):\n",
    "    result = o3d.registration.registration_fast_based_on_feature_matching(\n",
    "        source_down, target_down, source_fpfh, target_fpfh,\n",
    "        o3d.registration.FastGlobalRegistrationOption(\n",
    "            maximum_correspondence_distance=distance_threshold))\n",
    "    return result\n",
    "\n",
    "print(\"FPFH:\", np.asarray(source_fpfh.data).shape)\n",
    "#source_fpfh = o3d.registration.Feature()\n",
    "#source_fpfh.data = source_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast global registration took 0.077 sec.\n",
      "\n",
      "registration::RegistrationResult with fitness=4.200000e-02, inlier_rmse=7.122964e-03, and correspondence_set size of 42\n",
      "Access transformation to get result.\n",
      "[[ 1.   -0.    0.    0.12]\n",
      " [-0.    1.   -0.    0.24]\n",
      " [-0.   -0.    1.    0.38]\n",
      " [-0.    0.   -0.    1.  ]]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "result_fast = execute_fast_global_registration(source_down, target_down,\n",
    "                                               source_fpfh, target_fpfh,\n",
    "                                               voxel_size)\n",
    "print(\"Fast global registration took %.3f sec.\\n\" % (time.time() - start))\n",
    "print(result_fast)\n",
    "\n",
    "\n",
    "tf = np.asarray(result_fast.transformation)\n",
    "print(tf)\n",
    "\n",
    "vis[\"scene\"].set_object(\n",
    "    g.PointCloud(position=np.asarray(scene_o3d.points).T,\n",
    "                 color=np.asarray(scene_o3d.colors).T,\n",
    "                 size=0.01))\n",
    "model_pts_tf = (np.dot(tf[:3, :3], np.asarray(model_o3d.points).T).T + tf[:3, 3]).T\n",
    "vis[\"model\"].set_object(\n",
    "    g.PointCloud(position=model_pts_tf,\n",
    "                 color=np.asarray(model_o3d.colors).T,\n",
    "                 size=0.01))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
